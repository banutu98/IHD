{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from enum import Enum\nimport os\n\nTRAIN_DIR = os.path.join(os.getcwd(), 'data', 'train', '')\nTEST_DIR = os.path.join(os.getcwd(), 'data', 'test', '')\nCSV_FILENAME = 'Submission.csv'\n\nclass HemorrhageTypes(Enum):\n    EP = \"epidural\"\n    IN_PA = \"intraparenchymal\"\n    IN_VE = \"intraventricular\"\n    SUB_AR = \"subarachnoid\"\n    SUB_DU = \"subdural\"\n    ANY = \"any\"\n\n\n# There are at least 5 windows that a radiologist goes through for each scan!\n# Brain Matter window : W:80 L:40\n# Blood/subdural window: W:130-300 L:50-100\n# Soft tissue window: W:350–400 L:20–60\n# Bone window: W:2800 L:600\n# Grey-white differentiation window: W:8 L:32 or W:40 L:40\nBRAIN_MATTER_WINDOW = (40, 80)\nSUBDURAL_WINDOW = (80, 200)\nSOFT_TISSUE_WINDOW = (40, 380)\nBONE_WINDOW = (600, 2800)\nGRAY_WHITE_DIFFERENTIATION_WINDOW = (40, 40)\n\nALL_WINDOW_VALUES = {'BRAIN_MATTER': BRAIN_MATTER_WINDOW,\n                     'SUBDURAL': SUBDURAL_WINDOW,\n                     'SOFT_TISSUE': SOFT_TISSUE_WINDOW,\n                     'BONE': BONE_WINDOW,\n                     'GRAY_WHITE': GRAY_WHITE_DIFFERENTIATION_WINDOW}\n\nKERNEL_WIDTH = 13\nKERNEL_HEIGHT = 13\nGAUSS_MEAN = 0.1\nGAUSS_STDDEV = 0.05\nBRIGHTNESS_DELTA = 0.4","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"def create_output_csv(output_dict):\n    content = \"ID,Label\\n\"\n    for image_id in output_dict:\n        for num, hemorrhageType in enumerate(HemorrhageTypes, start=0):\n            content += create_output_line(image_id, hemorrhageType.value, output_dict[image_id][num])\n    with open(CSV_FILENAME, \"w\") as f:\n        f.write(content)\n\n\ndef create_output_line(image_id, hemorrhage_type, probability):\n    return \"ID_\" + image_id + \"_\" + hemorrhage_type + \",\" + str(probability) + \"\\n\"\n","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport glob\nimport pydicom\nimport numpy as np\n\n\ndef get_sequence_clipping_order(seq_length):\n    indices = []\n    elem = 0\n    for idx, i in enumerate(reversed(range(seq_length))):\n        indices.append(elem)\n        if idx % 2 == 0:\n            elem += i\n        else:\n            elem -= i\n    return indices\n\n\ndef print_error(message):\n    c_red = '\\033[95m'\n    c_end = '\\033[0m'\n    print(c_red + message + c_end)\n\n\ndef get_csv_train(data_prefix=TRAIN_DIR):\n    train_df = pd.read_csv('../input/rsna-intracranial-hemorrhage-detection/stage_1_train.csv')\n    train_df[['ID', 'subtype']] = train_df['ID'].str.rsplit('_', 1,\n                                                            expand=True)\n    train_df = train_df.rename(columns={'ID': 'id', 'Label': 'label'})\n    train_df = pd.pivot_table(train_df, index='id',\n                              columns='subtype', values='label')\n    #train_df.to_csv(\"labels.csv\")\n    return train_df\n\n\ndef extract_csv_partition():\n    df = get_csv_train()\n    meta_data_train = combine_labels_metadata(TRAIN_DIR)\n    negative, positive = df.loc[df['any'] == 0], df.loc[df['any'] == 1]\n    negative_study_uids = list(meta_data_train.query(\"any == 0\")['StudyInstanceUID'])\n    indices = np.arange(min(len(negative_study_uids), len(positive.index)))\n    np.random.shuffle(indices)\n    negative_study_uids = np.array(negative_study_uids)[indices]\n    selected_negative_studies = meta_data_train.loc[meta_data_train['StudyInstanceUID'].isin(negative_study_uids)]\n    selected_negative_studies = selected_negative_studies.drop(\n        set(selected_negative_studies.columns).intersection(set(negative.columns)), axis=1)\n    negative = negative.merge(selected_negative_studies, how='left', on='id').dropna()\n    negative = negative.drop(selected_negative_studies.columns, axis=1)\n    return pd.concat([positive, negative])\n\n\ndef extract_metadata(data_prefix=TRAIN_DIR):\n    filenames = glob.glob(\"../input/rsna-intracranial-hemorrhage-detection/stage_1_train_images/*.dcm\")\n    get_id = lambda p: os.path.splitext(os.path.basename(p))[0]\n    ids = map(get_id, filenames)\n    dcms = map(pydicom.dcmread, filenames)\n    columns = ['BitsAllocated', 'BitsStored', 'Columns', 'HighBit',\n               'Modality', 'PatientID', 'PhotometricInterpretation',\n               'PixelRepresentation', 'RescaleIntercept', 'RescaleSlope',\n               'Rows', 'SOPInstanceUID', 'SamplesPerPixel', 'SeriesInstanceUID',\n               'StudyID', 'StudyInstanceUID', 'ImagePositionPatient',\n               'ImageOrientationPatient', 'PixelSpacing']\n    meta_dict = {col: [] for col in columns}\n    for img in dcms:\n        for col in columns:\n            meta_dict[col].append(getattr(img, col))\n    meta_df = pd.DataFrame(meta_dict)\n    del meta_dict\n    meta_df['id'] = pd.Series(ids, index=meta_df.index)\n    split_cols = ['ImagePositionPatient1', 'ImagePositionPatient2',\n                  'ImagePositionPatient3', 'ImageOrientationPatient1',\n                  'ImageOrientationPatient2', 'ImageOrientationPatient3',\n                  'ImageOrientationPatient4', 'ImageOrientationPatient5',\n                  'ImageOrientationPatient6', 'PixelSpacing1',\n                  'PixelSpacing2']\n    meta_df[split_cols[:3]] = pd.DataFrame(meta_df.ImagePositionPatient.values.tolist())\n    meta_df[split_cols[3:9]] = pd.DataFrame(meta_df.ImageOrientationPatient.values.tolist())\n    meta_df[split_cols[9:]] = pd.DataFrame(meta_df.PixelSpacing.values.tolist())\n    meta_df = meta_df.drop(['ImagePositionPatient', 'ImageOrientationPatient', 'PixelSpacing'], axis=1)\n    return meta_df\n\n\ndef combine_labels_metadata(data_prefix=TRAIN_DIR):\n    meta_df = extract_metadata(data_prefix)\n    df = get_csv_train(data_prefix)\n    df = df.merge(meta_df, how='left', on='id').dropna()\n    df.sort_values(by='ImagePositionPatient3', inplace=True, ascending=False)\n    df.to_csv(os.path.join(data_prefix, 'train_meta.csv'))\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"import cv2\n\n\n# the kernel sizes must be positive odd integers but they do not have to be equal\n# the larger they are the more the image will be blurred\n\n\ndef blur_image(pixel_matrix, kernel_size_width=KERNEL_WIDTH, kernel_size_height=KERNEL_HEIGHT):\n    return cv2.GaussianBlur(pixel_matrix, (kernel_size_width, kernel_size_height), cv2.BORDER_DEFAULT)\n\n\ndef noisy(image, mean=GAUSS_MEAN, stddev=GAUSS_STDDEV):\n    gauss = np.random.normal(mean, stddev, image.shape)\n    noisy = image + gauss\n    noisy_min = np.amin(noisy)\n    noisy_max = np.amax(noisy)\n    noisy = (noisy - noisy_min) / (noisy_max - noisy_min)\n    return noisy\n\n\ndef adjust_brightness(image, delta=BRIGHTNESS_DELTA):\n    image += delta\n    image[image < 0] = 0\n    image[image > 1] = 1\n    return image","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"import copy\n\nimport pydicom\nimport scipy\nfrom skimage import morphology\n\n\nclass Preprocessor:\n\n    @staticmethod\n    def apply_hounsfield(image, intercept, slope):\n        if slope is not 1:\n            image = slope * image.astype(np.float64)\n            image = image.astype(np.float64)\n\n        image += np.float64(intercept)\n\n        # Setting values smaller than air, to air. Values smaller than -1024, are probably just outside the scanner.\n        image[image < -1024] = -1024\n        return image\n\n    @staticmethod\n    def windowing(image, custom_center=30, custom_width=100, rescale=True):\n        new_image = copy.deepcopy(image)\n        min_value = custom_center - (custom_width / 2)\n        max_value = custom_center + (custom_width / 2)\n\n        # Including another value for values way outside the range, to (hopefully) make segmentation processes easier.\n        new_image[new_image < min_value] = min_value\n        new_image[new_image > max_value] = max_value\n        if rescale:\n            new_image = (new_image - min_value) / (max_value - min_value)\n        return new_image\n\n    @staticmethod\n    def image_resample(image, pixel_spacing, new_spacing=[1, 1]):\n        pixel_spacing = map(float, pixel_spacing)\n        spacing = np.array(list(pixel_spacing))\n        resize_factor = spacing / new_spacing\n        new_real_shape = image.shape * resize_factor\n        new_shape = np.round(new_real_shape)\n        real_resize_factor = new_shape / image.shape\n\n        image = scipy.ndimage.interpolation.zoom(image, real_resize_factor)\n        return image\n\n    @staticmethod\n    def image_background_segmentation(image, WL=30, WW=100, rescale=True):\n        lB = WW - WL\n        uB = WW + WL\n\n        # Keep only values inside of the window\n        background_separation = np.logical_and(image > lB, image < uB)\n\n        # Get largest connected component:\n        # From https://github.com/nilearn/nilearn/blob/master/nilearn/_utils/ndimage.py\n        background_separation = morphology.dilation(background_separation, np.ones((5, 5)))\n        labels, label_nb = scipy.ndimage.label(background_separation)\n\n        label_count = np.bincount(labels.ravel().astype(np.int))\n        # discard the 0 label\n        label_count[0] = 0\n        mask = labels == label_count.argmax()\n\n        # Fill holes in the mask\n        mask = morphology.dilation(mask, np.ones((5, 5)))  # dilate the mask for less fuzy edges\n        mask = scipy.ndimage.morphology.binary_fill_holes(mask)\n        mask = morphology.dilation(mask, np.ones((3, 3)))  # dilate the mask again\n\n        image = mask * image\n\n        if rescale:\n            img_min = np.amin(image)\n            img_max = np.amax(image)\n            image = (image - img_min) / (img_max - img_min)\n        return image\n\n    @staticmethod\n    def preprocess(image_path):\n        dicom = pydicom.read_file(image_path)\n        image = dicom.pixel_array.astype(np.float64)\n        p = Preprocessor\n        image = p.apply_hounsfield(image, dicom.RescaleIntercept, dicom.RescaleSlope)\n        image = p.windowing(image)\n        return image\n\n    @staticmethod\n    def augment(image):\n        augmented = list()\n        augmented.append(blur_image(image))\n        augmented.append(noisy(image))\n        augmented.append(adjust_brightness(image, 0.3))\n        return augmented","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"import os\nimport random\n\nfrom keras.utils import Sequence\n\n\nclass DataGenerator(Sequence):\n\n    def __init__(self, list_ids, labels=None, batch_size=1, img_size=(512, 512, 3),\n                 img_dir='../input/rsna-intracranial-hemorrhage-detection/stage_1_train_images/', shuffle=True, n_classes=2):\n        self.list_ids = list_ids\n        self.indices = np.arange(len(self.list_ids))\n        self.labels = labels\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.img_dir = img_dir\n        self.shuffle = shuffle\n        self.n_classes = n_classes\n        # TODO: this could be generalized with the help of\n        # an Augmenter class\n        self.n_augment = 3  # 3 data augmentation functions\n        self.augment_funcs = [blur_image,\n                              noisy,\n                              adjust_brightness,\n                              lambda img: img]  # identity function\n        self.on_epoch_end()\n        if labels is not None:\n            # Weights should be a probability distribution.\n            # If the number of training instances is too large,\n            # there could be issues! (arithmetic underflow)\n            weight_func = lambda row: 1.0 if row[\"any\"] == 0 else self.n_augment + 1\n            self.weights = labels.apply(weight_func, axis=1)\n            total = self.weights.sum()\n            self.weights = (self.weights / total).values\n        # set random seed, hope this randomizes starting value for\n        # each worker\n        random.seed(os.urandom(8))\n\n    def __len__(self):\n        return len(self.indices) // self.batch_size\n\n    def __getitem__(self, index):\n        indices = np.random.choice(self.indices, size=self.batch_size,\n                                   replace=False, p=self.weights)\n        return self.__data_generation(indices)\n\n    # Don't think this is necessary anymore, indices are sampled randomly.\n    def on_epoch_end(self):\n        pass\n\n    def __data_generation(self, indices):\n        x = np.empty((self.batch_size, *self.img_size))\n        if self.labels is not None:  # training phase\n            y = np.empty((self.batch_size, self.n_classes), dtype=np.float32)\n            for i, idx in enumerate(indices):\n                #print(\"i =\", i)\n                #print(\"idx =\", idx)\n                #print(\"type self.list_ids =\", type(self.list_ids))\n                #print(\"self.list_ids[idx] =\", self.list_ids[idx])\n                image = Preprocessor.preprocess(self.img_dir + self.list_ids[idx] + \".dcm\")\n                if self.labels.iloc[idx]['any'] == 1:\n                    image = self.augment_funcs[random.randint(0, self.n_augment)](image)\n                image = np.array(image)\n                image = np.repeat(image[..., np.newaxis], 3, -1)\n                x[i, ] = image\n                if self.n_classes == 2:\n                    y[i, ] = self.labels.iloc[idx]['any']\n                else:\n                    y[i, ] = self.labels.iloc[idx, 1:]\n            return x, y\n        else:  # test phase\n            for i, idx in enumerate(indices):\n                image = Preprocessor.preprocess(self.img_dir + self.list_ids[idx] + \".dcm\")\n                image = np.repeat(image[..., np.newaxis], 3, -1)\n                x[i, ] = image\n            return x","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"import os\nfrom keras.utils import Sequence\n\n\nclass LSTMDataGenerator(Sequence):\n\n    def __init__(self, list_ids, labels=None, batch_size=1, img_size=(512, 512, 3),\n                 sequence_size=20, img_dir='../input/rsna-intracranial-hemorrhage-detection/stage_1_train_images/', shuffle=True):\n        # here, list_ids is a series of lists; each list represents an\n        # ordered sequence of scans that compose a single study\n        self.list_ids = list_ids\n        self.indices = np.arange(len(self.list_ids))\n        self.labels = labels\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.sequence_size = sequence_size\n        self.img_dir = img_dir\n        self.shuffle = shuffle\n        # TODO: this could be generalized with the help of\n        # an Augmenter class\n        self.n_augment = 3  # 3 data augmentation functions\n        self.augment_funcs = [blur_image,\n                              noisy,\n                              adjust_brightness,\n                              lambda img: img]  # identity function\n        self.on_epoch_end()\n        if labels is not None:\n            # Weights should be a probability distribution.\n            # If the number of training instances is too large,\n            # there could be issues! (arithmetic underflow)\n            weight_func = lambda seq: (float(self.n_augment + 1)\n                                       if any([labels[0] for labels in seq])\n                                       else 1.0)\n            self.weights = np.array(list(map(weight_func, self.labels)))\n            total = np.sum(self.weights)\n            self.weights = (self.weights / total)\n\n    def __len__(self):\n        return len(self.indices) // self.batch_size\n\n    def __getitem__(self, index):\n        indices = np.random.choice(self.indices, size=self.batch_size,\n                                   replace=False, p=self.weights)\n        return self.__data_generation(indices)\n\n    def on_epoch_end(self):\n        pass\n\n    def __data_generation(self, indices):\n        x = np.empty((self.batch_size, self.sequence_size, *self.img_size))\n        preprocess_func = lambda im: Preprocessor.preprocess(os.path.join(self.img_dir, im + \".dcm\"))\n        if self.labels is not None:  # training phase\n            y = np.empty((self.batch_size, self.sequence_size, 5), dtype=np.float32)\n            for i, idx in enumerate(indices):\n                seq = self.list_ids[idx]\n                seq_labels = np.array(self.labels[idx])\n                seq_len = len(seq)\n                # if there is an any label = 1, set has_hemorrhage flag\n                has_hemorrhage = np.any(seq_labels[0])\n                imgs = map(preprocess_func, seq)\n                if has_hemorrhage:\n                    # augment images\n                    func_idxs = np.random.randint(0, self.n_augment + 1,\n                                                  size=seq_len)\n                    imgs = [self.augment_funcs[j](img)\n                            for j, img in zip(func_idxs, imgs)]\n                else:\n                    # consume map generator\n                    imgs = list(imgs)\n                imgs = np.array(imgs)\n                imgs = np.repeat(imgs[..., np.newaxis], 3, -1)\n                diff = seq_len - self.sequence_size\n                if diff < 0:\n                    padding = np.repeat(np.zeros(imgs.shape[1:])[np.newaxis, ...], abs(diff), 0)\n                    imgs = np.concatenate((imgs, padding), axis=0)\n                    seq_labels = np.concatenate(seq_labels, np.zeros(6), axis=0)\n                elif diff > 0:\n                    indices = get_sequence_clipping_order(seq_len)\n                    imgs = np.delete(imgs, indices[:diff], 0)\n                    seq_labels = np.delete(seq_labels, indices[:diff], 0)\n                x[i, ] = imgs\n                y[i, ] = seq_labels[:, 1:]\n            return x, y\n        else:  # test phase\n            for i, idx in enumerate(indices):\n                seq = self.list_ids[idx]\n                seq_len = len(seq)\n                imgs = np.array(list(map(preprocess_func, seq)))\n                imgs = np.repeat(imgs[..., np.newaxis], 3, -1)\n                diff = seq_len - self.sequence_size\n                if diff < 0:\n                    padding = np.repeat(np.zeros(imgs.shape[1:])[np.newaxis, ...], abs(diff), 0)\n                    imgs = np.concatenate((imgs, padding), axis=0)\n                elif diff > 0:\n                    indices = get_sequence_clipping_order(seq_len)\n                    imgs = np.delete(imgs, indices[:diff], 0)\n                x[i, ] = imgs\n            return x","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"from keras.applications import NASNetLarge, InceptionResNetV2, Xception, DenseNet201, ResNet50\nfrom keras.layers import Bidirectional, LSTM, TimeDistributed, Masking\nfrom keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input, GlobalAveragePooling2D\nfrom keras.models import Sequential, Model\n\n\nclass StandardModel:\n\n    def __init__(self, network, input_shape, pooling_method='max', classes=2, use_softmax=True):\n        self.base_model = self.get_base_model(network, input_shape, pooling_method)\n        self.classes = classes\n        self.use_softmax = use_softmax\n        self.input_shape = input_shape\n\n    @staticmethod\n    def get_base_model(network, input_shape, pooling_method):\n        network = network.lower()\n        input_warning_message = 'WARNING! The input shape is not the default one!!! Proceeding anyway!'\n        if network == 'nas':\n            if input_shape != (331, 331, 3):\n                print_error(input_warning_message)\n            return NASNetLarge(input_shape=input_shape, include_top=False, pooling=pooling_method,\n                               weights=None)\n        elif network == 'inception':\n            if input_shape != (299, 299, 3):\n                print_error(input_warning_message)\n            return InceptionResNetV2(input_shape=input_shape, include_top=False, pooling=pooling_method,\n                                     weights='imagenet')\n        elif network == 'xception':\n            if input_shape != (299, 299, 3):\n                print_error(input_warning_message)\n            return Xception(input_shape=input_shape, include_top=False, pooling=pooling_method,\n                            weights='imagenet')\n        elif network == 'densenet':\n            if input_shape != (224, 224, 3):\n                print_error(input_warning_message)\n            return DenseNet201(input_shape=input_shape, include_top=False, pooling=pooling_method,\n                               weights='imagenet')\n        elif network == 'resnet':\n            if input_shape != (224, 224, 3):\n                print_error(input_warning_message)\n            return ResNet50(input_shape=input_shape, include_top=False, pooling=pooling_method,\n                            weights='imagenet')\n        else:\n            print_error(f'Invalid network name: {network}! Please choose from: \\n ')\n            return None\n\n    def build_model(self):\n        return self.build_binary_model() if self.classes == 2 else self.build_multi_class_model()\n\n    def build_binary_model(self):\n        model = Sequential()\n        model.add(self.base_model)\n        model.add(Dense(96))\n        model.add(BatchNormalization())\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(Dropout(0.3))\n        model.add(Dense(2, activation='softmax'))\n        return model\n\n    def build_multi_class_model(self):\n        return self.build_probability_model() if self.use_softmax else self.build_recurrent_model()\n\n    def build_probability_model(self):\n        model = Sequential()\n        model.add(self.base_model)\n        model.add(Dense(96))\n        model.add(BatchNormalization())\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(Dropout(0.3))\n        model.add(Dense(5, activation='softmax'))\n        return model\n\n    def build_recurrent_model(self):\n        inputs = Input(shape=(3, *self.input_shape))\n        time_dist = TimeDistributed(self.base_model)(inputs)\n        global_pool = TimeDistributed(GlobalAveragePooling2D())(time_dist)\n        dense_relu = TimeDistributed(Dense(256, activation='relu'))(global_pool)\n\n        masked = Masking(0.0)(dense_relu)\n        out = Bidirectional(LSTM(256, return_sequences=True, activation='softsign',\n                                 dropout=0.2, recurrent_dropout=0.2))(masked)\n        out = TimeDistributed(Dense(5, activation='softmax'))(out)\n\n        model = Model(inputs=inputs, outputs=out)\n        return model","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"is_executing":true,"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"import glob\nimport os\nimport sys\n\nimport keras\nfrom keras.optimizers import Adamax\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom sklearn.metrics import log_loss\n\n\ndef prepare_data():\n    csv = pd.read_csv('../input/labels/labels.csv')\n    files = glob.glob(os.path.join(\"../input/rsna-intracranial-hemorrhage-detection/stage_1_train_images/*.dcm\"))\n    files = list(map(lambda x: os.path.splitext(os.path.basename(x))[0], files))\n    filtered_csv = csv[csv.id.isin(files)]\n    indices = np.random.rand(len(filtered_csv))\n    mask = indices < 0.2\n    x_train, y_train = filtered_csv[mask].id, filtered_csv.iloc[mask, 1:]\n    x_test, y_test = filtered_csv[~mask].id, filtered_csv.iloc[~mask, 1:]\n    x_train.reset_index(inplace=True, drop=True)\n    y_train.reset_index(inplace=True, drop=True)\n    x_test.reset_index(inplace=True, drop=True)\n    y_test.reset_index(inplace=True, drop=True)\n    return x_train, y_train, x_test, y_test\n\n\ndef prepare_sequential_data():\n    # open label + metadata CSV\n    csv = pd.read_csv(\"../input/train_meta/train_meta.csv\")\n    # sort by study ID and position\n    csv.sort_values(by=[\"StudyInstanceUID\", \"ImagePositionPatient3\"], inplace=True, ascending=False)\n    label_columns = [\"any\", \"epidural\", \"intraparenchymal\",\n                     \"intraventricular\", \"subarachnoid\", \"subdural\"]\n    # filter unnecessary columns\n    csv = csv[[\"StudyInstanceUID\", \"id\"] + label_columns]\n    # get sequences of IDs (groupby preserves order)\n    sequences = csv.groupby(\"StudyInstanceUID\")[\"id\"].apply(list)\n    # group labels into one single column\n    csv[\"labels\"] = csv[label_columns].values.tolist()\n    # get sequences of labels\n    labels = csv.groupby(\"StudyInstanceUID\")[\"labels\"].apply(list)\n    indices = np.random.rand(sequences.size)\n    # partition data\n    mask = indices < 0.1\n    x_train, x_test = list(sequences.iloc[mask]), list(sequences.iloc[~mask])\n    y_train, y_test = list(labels.iloc[mask]), list(labels.iloc[~mask])\n    return x_train, y_train, x_test, y_test\n\n\ndef test_recurrent_network():\n    def generate_single_instance(instance):\n        images, labels = list(), list()\n        for file in instance:\n            file_path = os.path.join(\"../input/rsna-intracranial-hemorrhage-detection/stage_1_train_images\", file)\n            images.append(Preprocessor.preprocess(file_path))\n            labels.append(np.random.uniform(0, 1, 5))\n        images = np.stack(images, axis=0)\n        labels = np.stack(labels, axis=0)\n        return images, labels\n\n    model = StandardModel('xception', (512, 512, 3), classes=5, use_softmax=False, pooling_method=None)\n    model = model.build_model()\n    model.compile(Adamax(), loss='categorical_crossentropy', metrics=['acc'])\n    model.summary()\n    keras.utils.plot_model(model, show_shapes=True)\n    x_train = []\n    y_train = []\n    data = [['ID_00025ef4b.dcm', 'ID_00027c277.dcm', 'ID_00027cbb1.dcm'],\n            ['ID_000229f2a.dcm', 'ID_000230ed7.dcm', 'ID_000270f8b.dcm'],\n            ['ID_00025ef4b.dcm', 'ID_00027c277.dcm', 'ID_00027cbb1.dcm']]\n    for i in range(3):\n        instance_images, instance_labels = generate_single_instance(data[i])\n        x_train.append(instance_images)\n        y_train.append(instance_labels)\n    x_train = np.stack(x_train)\n    x_train = np.repeat(x_train[..., np.newaxis], 3, -1)\n    y_train = np.stack(y_train)\n    print(x_train.shape, y_train.shape)\n    model.fit(x_train, y_train, batch_size=1)\n\n\ndef train_binary_model(base_model, already_trained_model=None):\n    x_train, y_train, x_test, y_test = prepare_data()\n    if not already_trained_model:\n        filepath=\"binary_weights.best.hdf5\"\n        checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n        early = EarlyStopping(monitor='val_loss', patience=3)\n        callbacks_list = [checkpoint, early]\n        model = StandardModel(base_model, (512, 512, 3), classes=2, use_softmax=True)\n        model = model.build_model()\n        model.compile(Adamax(), loss='categorical_crossentropy', metrics=['acc'])\n        model.fit_generator(DataGenerator(x_train, labels=y_train, n_classes=2, batch_size=8), callbacks=callbacks_list, epochs=10, workers=1,\n                            validation_data=DataGenerator(x_test, labels=y_test, n_classes=2, batch_size=8))\n        #model.save('model.h5')\n        loss, accuracy = model.evaluate_generator(DataGenerator(x_test, labels=y_test, n_classes=2))\n        print(loss, accuracy)\n    else:\n        if os.path.exists(already_trained_model):\n            model = keras.models.load_model(already_trained_model)\n            loss, accuracy = model.evaluate_generator(DataGenerator(x_test, labels=y_test, n_classes=2))\n            print(loss, accuracy)\n        else:\n            print_error(\"Provided model file doesn't exist! Exiting...\")\n            sys.exit(1)\n\n\ndef train_multi_class_model(base_model, already_trained_model=None):\n    x_train, y_train, x_test, y_test = prepare_data()\n    if not already_trained_model:\n        model = StandardModel(base_model, (512, 512, 3), classes=5, use_softmax=True)\n        model = model.build_model()\n        model.compile(Adamax(), loss='categorical_crossentropy', metrics=['acc'])\n        model.fit_generator(DataGenerator(x_train, labels=y_train, n_classes=5))\n        model.save('model.h5')\n        y_pred = model.predict_generator(DataGenerator(x_test, n_classes=5))\n        y_test = y_test.iloc[:, 1:]\n        print(log_loss(y_test, y_pred))\n    else:\n        if os.path.exists(already_trained_model):\n            model = keras.models.load_model(already_trained_model)\n            y_pred = model.predict_generator(DataGenerator(x_test, n_classes=5))\n            y_test = y_test.iloc[:, 1:]\n            print(log_loss(y_test, y_pred))\n        else:\n            print_error(\"Provided model file doesn't exist! Exiting...\")\n            sys.exit(1)\n\n\ndef train_recurrent_multi_class_model(base_model, already_trained_model=None):\n    x_train, y_train, x_test, y_test = prepare_sequential_data()\n    if not already_trained_model:\n        model = StandardModel(base_model, (512, 512, 3), classes=5, use_softmax=False, pooling_method=None)\n        model = model.build_model()\n        model.compile(Adamax(), loss='categorical_crossentropy', metrics=['acc'])\n        model.fit_generator(LSTMDataGenerator(x_train, labels=y_train))\n        model.save('model.h5')\n        y_pred = model.predict_generator(LSTMDataGenerator(x_test))\n        print(log_loss(y_test, y_pred))\n    else:\n        if os.path.exists(already_trained_model):\n            model = keras.models.load_model(already_trained_model)\n            y_pred = model.predict_generator(LSTMDataGenerator(x_test))\n            print(log_loss(y_test, y_pred))\n        else:\n            print_error(\"Provided model file doesn't exist! Exiting...\")\n            sys.exit(1)\n\n\ndef main():\n    # TODO: Possible MODELS for training: inception, xception, resnet, densenet, nas\n    # train_binary_model('xception')\n    # train_multi_class_model('densenet')\n    # prepare_sequential_data()\n    train_binary_model('xception')\n    #test_recurrent_network()\n\nmain()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}}},"nbformat":4,"nbformat_minor":1}