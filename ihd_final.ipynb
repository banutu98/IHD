{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from enum import Enum\nimport os\n\nTRAIN_DIR = os.path.join('../input', 'rsna-intracranial-hemorrhage-detection', 'stage_1_train_images', '')\nTEST_DIR = os.path.join('../input', 'rsna-intracranial-hemorrhage-detection', 'stage_1_test_images', '')\nCSV_FILENAME = 'Submission.csv'\n\nclass HemorrhageTypes(Enum):\n    EP = \"epidural\"\n    IN_PA = \"intraparenchymal\"\n    IN_VE = \"intraventricular\"\n    SUB_AR = \"subarachnoid\"\n    SUB_DU = \"subdural\"\n    ANY = \"any\"\n\n\n# There are at least 5 windows that a radiologist goes through for each scan!\n# Brain Matter window : W:80 L:40\n# Blood/subdural window: W:130-300 L:50-100\n# Soft tissue window: W:350–400 L:20–60\n# Bone window: W:2800 L:600\n# Grey-white differentiation window: W:8 L:32 or W:40 L:40\nBRAIN_MATTER_WINDOW = (40, 80)\nSUBDURAL_WINDOW = (80, 200)\nSOFT_TISSUE_WINDOW = (40, 380)\nBONE_WINDOW = (600, 2800)\nGRAY_WHITE_DIFFERENTIATION_WINDOW = (40, 40)\n\nALL_WINDOW_VALUES = {'BRAIN_MATTER': BRAIN_MATTER_WINDOW,\n                     'SUBDURAL': SUBDURAL_WINDOW,\n                     'SOFT_TISSUE': SOFT_TISSUE_WINDOW,\n                     'BONE': BONE_WINDOW,\n                     'GRAY_WHITE': GRAY_WHITE_DIFFERENTIATION_WINDOW}\n\nKERNEL_WIDTH = 13\nKERNEL_HEIGHT = 13\nGAUSS_MEAN = 0.1\nGAUSS_STDDEV = 0.05\nBRIGHTNESS_DELTA = 0.4","execution_count":151,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"def create_output_csv(output_dict):\n    content = \"ID,Label\\n\"\n    for image_id in output_dict:\n        for num, hemorrhageType in enumerate(HemorrhageTypes, start=0):\n            content += create_output_line(image_id, hemorrhageType.value, output_dict[image_id][num])\n    with open(CSV_FILENAME, \"w\") as f:\n        f.write(content)\n\n\ndef create_output_line(image_id, hemorrhage_type, probability):\n    return \"ID_\" + image_id + \"_\" + hemorrhage_type + \",\" + str(probability) + \"\\n\"\n","execution_count":152,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport glob\nimport pydicom\nimport numpy as np\n\n\ndef get_sequence_clipping_order(seq_length):\n    indices = []\n    elem = 0\n    for idx, i in enumerate(reversed(range(seq_length))):\n        indices.append(elem)\n        if idx % 2 == 0:\n            elem += i\n        else:\n            elem -= i\n    return indices\n\n\ndef print_error(message):\n    c_red = '\\033[95m'\n    c_end = '\\033[0m'\n    print(c_red + message + c_end)\n\n\ndef get_csv_train(data_prefix=TRAIN_DIR):\n    train_df = pd.read_csv(os.path.join(data_prefix, 'stage_1_train.csv'))\n    train_df[['ID', 'subtype']] = train_df['ID'].str.rsplit('_', 1,\n                                                            expand=True)\n    train_df = train_df.rename(columns={'ID': 'id', 'Label': 'label'})\n    train_df = pd.pivot_table(train_df, index='id',\n                              columns='subtype', values='label')\n    train_df.to_csv(\"labels.csv\")\n    return train_df\n\n\ndef extract_csv_partition():\n    df = get_csv_train()\n    meta_data_train = combine_labels_metadata(TRAIN_DIR)\n    negative, positive = df.loc[df['any'] == 0], df.loc[df['any'] == 1]\n    negative_study_uids = list(meta_data_train.query(\"any == 0\")['StudyInstanceUID'])\n    indices = np.arange(min(len(negative_study_uids), len(positive.index)))\n    np.random.shuffle(indices)\n    negative_study_uids = np.array(negative_study_uids)[indices]\n    selected_negative_studies = meta_data_train.loc[meta_data_train['StudyInstanceUID'].isin(negative_study_uids)]\n    selected_negative_studies = selected_negative_studies.drop(\n        set(selected_negative_studies.columns).intersection(set(negative.columns)), axis=1)\n    negative = negative.merge(selected_negative_studies, how='left', on='id').dropna()\n    negative = negative.drop(selected_negative_studies.columns, axis=1)\n    return pd.concat([positive, negative])\n\n\ndef extract_metadata(data_prefix=TRAIN_DIR):\n    filenames = glob.glob(os.path.join(data_prefix, \"*.dcm\"))\n    get_id = lambda p: os.path.splitext(os.path.basename(p))[0]\n    ids = map(get_id, filenames)\n    dcms = map(pydicom.dcmread, filenames)\n    columns = ['BitsAllocated', 'BitsStored', 'Columns', 'HighBit',\n               'Modality', 'PatientID', 'PhotometricInterpretation',\n               'PixelRepresentation', 'RescaleIntercept', 'RescaleSlope',\n               'Rows', 'SOPInstanceUID', 'SamplesPerPixel', 'SeriesInstanceUID',\n               'StudyID', 'StudyInstanceUID', 'ImagePositionPatient',\n               'ImageOrientationPatient', 'PixelSpacing']\n    meta_dict = {col: [] for col in columns}\n    for img in dcms:\n        for col in columns:\n            meta_dict[col].append(getattr(img, col))\n    meta_df = pd.DataFrame(meta_dict)\n    del meta_dict\n    meta_df['id'] = pd.Series(ids, index=meta_df.index)\n    split_cols = ['ImagePositionPatient1', 'ImagePositionPatient2',\n                  'ImagePositionPatient3', 'ImageOrientationPatient1',\n                  'ImageOrientationPatient2', 'ImageOrientationPatient3',\n                  'ImageOrientationPatient4', 'ImageOrientationPatient5',\n                  'ImageOrientationPatient6', 'PixelSpacing1',\n                  'PixelSpacing2']\n    meta_df[split_cols[:3]] = pd.DataFrame(meta_df.ImagePositionPatient.values.tolist())\n    meta_df[split_cols[3:9]] = pd.DataFrame(meta_df.ImageOrientationPatient.values.tolist())\n    meta_df[split_cols[9:]] = pd.DataFrame(meta_df.PixelSpacing.values.tolist())\n    meta_df = meta_df.drop(['ImagePositionPatient', 'ImageOrientationPatient', 'PixelSpacing'], axis=1)\n    return meta_df\n\n\ndef combine_labels_metadata(data_prefix=TRAIN_DIR):\n    meta_df = extract_metadata(data_prefix)\n    df = get_csv_train(data_prefix)\n    df = df.merge(meta_df, how='left', on='id').dropna()\n    df.sort_values(by='ImagePositionPatient3', inplace=True, ascending=False)\n    df.to_csv(os.path.join(data_prefix, 'train_meta.csv'))\n    return df\n","execution_count":153,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"import cv2\n\n\n# the kernel sizes must be positive odd integers but they do not have to be equal\n# the larger they are the more the image will be blurred\n\n\ndef blur_image(pixel_matrix, kernel_size_width=KERNEL_WIDTH, kernel_size_height=KERNEL_HEIGHT):\n    return cv2.GaussianBlur(pixel_matrix, (kernel_size_width, kernel_size_height), cv2.BORDER_DEFAULT)\n\n\ndef noisy(image, mean=GAUSS_MEAN, stddev=GAUSS_STDDEV):\n    gauss = np.random.normal(mean, stddev, image.shape)\n    noisy = image + gauss\n    noisy_min = np.amin(noisy)\n    noisy_max = np.amax(noisy)\n    noisy = (noisy - noisy_min) / (noisy_max - noisy_min)\n    return noisy\n\n\ndef adjust_brightness(image, delta=BRIGHTNESS_DELTA):\n    image += delta\n    image[image < 0] = 0\n    image[image > 1] = 1\n    return image","execution_count":154,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"import copy\n\nimport pydicom\nimport scipy\nfrom skimage import morphology\nfrom skimage.transform import resize\n\n\nclass Preprocessor:\n\n    @staticmethod\n    def apply_hounsfield(image, intercept, slope):\n        if slope is not 1:\n            image = slope * image.astype(np.float64)\n            image = image.astype(np.float64)\n\n        image += np.float64(intercept)\n\n        # Setting values smaller than air, to air. Values smaller than -1024, are probably just outside the scanner.\n        image[image < -1024] = -1024\n        return image\n\n    @staticmethod\n    def windowing(image, custom_center=30, custom_width=100, rescale=True):\n        new_image = copy.deepcopy(image)\n        min_value = custom_center - (custom_width / 2)\n        max_value = custom_center + (custom_width / 2)\n\n        # Including another value for values way outside the range, to (hopefully) make segmentation processes easier.\n        new_image[new_image < min_value] = min_value\n        new_image[new_image > max_value] = max_value\n        if rescale:\n            new_image = (new_image - min_value) / (max_value - min_value)\n        return new_image\n\n    @staticmethod\n    def image_resample(image, pixel_spacing, new_spacing=[1, 1]):\n        pixel_spacing = map(float, pixel_spacing)\n        spacing = np.array(list(pixel_spacing))\n        resize_factor = spacing / new_spacing\n        new_real_shape = image.shape * resize_factor\n        new_shape = np.round(new_real_shape)\n        real_resize_factor = new_shape / image.shape\n\n        image = scipy.ndimage.interpolation.zoom(image, real_resize_factor)\n        return image\n\n    @staticmethod\n    def image_background_segmentation(image, WL=30, WW=100, rescale=True):\n        lB = WW - WL\n        uB = WW + WL\n\n        # Keep only values inside of the window\n        background_separation = np.logical_and(image > lB, image < uB)\n\n        # Get largest connected component:\n        # From https://github.com/nilearn/nilearn/blob/master/nilearn/_utils/ndimage.py\n        background_separation = morphology.dilation(background_separation, np.ones((5, 5)))\n        labels, label_nb = scipy.ndimage.label(background_separation)\n\n        label_count = np.bincount(labels.ravel().astype(np.int))\n        # discard the 0 label\n        label_count[0] = 0\n        mask = labels == label_count.argmax()\n\n        # Fill holes in the mask\n        mask = morphology.dilation(mask, np.ones((5, 5)))  # dilate the mask for less fuzy edges\n        mask = scipy.ndimage.morphology.binary_fill_holes(mask)\n        mask = morphology.dilation(mask, np.ones((3, 3)))  # dilate the mask again\n\n        image = mask * image\n\n        if rescale:\n            img_min = np.amin(image)\n            img_max = np.amax(image)\n            image = (image - img_min) / (img_max - img_min)\n        return image\n\n    @staticmethod\n    def preprocess(image_path):\n        dicom = pydicom.read_file(image_path)\n        image = dicom.pixel_array.astype(np.float64)\n        if image.shape != (512, 512):\n            image = resize(image, (512, 512))\n        p = Preprocessor\n        image = p.apply_hounsfield(image, dicom.RescaleIntercept, dicom.RescaleSlope)\n        image = p.windowing(image)\n        return image\n\n    @staticmethod\n    def augment(image):\n        augmented = list()\n        augmented.append(blur_image(image))\n        augmented.append(noisy(image))\n        augmented.append(adjust_brightness(image, 0.3))\n        return augmented","execution_count":155,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"import os\nimport random\n\nfrom keras.utils import Sequence\n\n\nclass DataGenerator(Sequence):\n\n    def __init__(self, list_ids, labels=None, batch_size=1, img_size=(512, 512, 3),\n                 img_dir=TRAIN_DIR, shuffle=True, n_classes=2):\n        self.list_ids = list_ids\n        self.indices = np.arange(len(self.list_ids))\n        self.labels = labels\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.img_dir = img_dir\n        self.shuffle = shuffle\n        self.n_classes = n_classes\n        # TODO: this could be generalized with the help of\n        # an Augmenter class\n        self.n_augment = 3  # 3 data augmentation functions\n        self.augment_funcs = [blur_image,\n                              noisy,\n                              adjust_brightness,\n                              lambda img: img]  # identity function\n        self.on_epoch_end()\n        if labels is not None:\n            # Weights should be a probability distribution.\n            # If the number of training instances is too large,\n            # there could be issues! (arithmetic underflow)\n            weight_func = lambda row: 1.0 if row[\"any\"] == 0 else self.n_augment + 1\n            self.weights = labels.apply(weight_func, axis=1)\n            total = self.weights.sum()\n            self.weights = (self.weights / total).values\n        # set random seed, hope this randomizes starting value for\n        # each worker\n        random.seed(os.urandom(8))\n\n    def __len__(self):\n        return len(self.indices) // self.batch_size\n\n    def __getitem__(self, index):\n        indices = np.random.choice(self.indices, size=self.batch_size,\n                                   replace=False, p=self.weights)\n        return self.__data_generation(indices)\n\n    # Don't think this is necessary anymore, indices are sampled randomly.\n    def on_epoch_end(self):\n        pass\n\n    def __data_generation(self, indices):\n        x = np.empty((self.batch_size, *self.img_size))\n        if self.labels is not None:  # training phase\n            if self.n_classes == 2:\n                y = np.empty((self.batch_size,), dtype=np.float32)\n            else:\n                y = np.empty((self.batch_size, self.n_classes), dtype=np.float32)\n            for i, idx in enumerate(indices):\n                image = Preprocessor.preprocess(self.img_dir + self.list_ids[idx] + \".dcm\")\n                if self.labels.iloc[idx]['any'] == 1:\n                    image = self.augment_funcs[random.randint(0, self.n_augment)](image)\n                image = np.array(image)\n                image = np.repeat(image[..., np.newaxis], 3, -1)\n                x[i, ] = image\n                if self.n_classes == 2:\n                    y[i, ] = self.labels.iloc[idx]['any']\n                else:\n                    y[i, ] = self.labels.iloc[idx, 1:]\n            return x, y\n        else:  # test phase\n            for i, idx in enumerate(indices):\n                image = Preprocessor.preprocess(self.img_dir + self.list_ids[idx] + \".dcm\")\n                image = np.repeat(image[..., np.newaxis], 3, -1)\n                x[i, ] = image\n            return x","execution_count":156,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"import os\nfrom keras.utils import Sequence\n\n\nclass LSTMDataGenerator(Sequence):\n\n    def __init__(self, list_ids, labels=None, batch_size=1, img_size=(512, 512, 3),\n                 sequence_size=40, img_dir='data/train', shuffle=True):\n        # here, list_ids is a series of lists; each list represents an\n        # ordered sequence of scans that compose a single study\n        self.list_ids = list_ids\n        self.indices = np.arange(len(self.list_ids))\n        self.labels = labels\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.sequence_size = sequence_size\n        self.img_dir = img_dir\n        self.shuffle = shuffle\n        # TODO: this could be generalized with the help of\n        # an Augmenter class\n        self.n_augment = 3  # 3 data augmentation functions\n        self.augment_funcs = [blur_image,\n                              noisy,\n                              adjust_brightness,\n                              lambda img: img]  # identity function\n        self.on_epoch_end()\n        if labels is not None:\n            # Weights should be a probability distribution.\n            # If the number of training instances is too large,\n            # there could be issues! (arithmetic underflow)\n            weight_func = lambda seq: (float(self.n_augment + 1)\n                                       if any([labels[0] for labels in seq])\n                                       else 1.0)\n            self.weights = np.array(list(map(weight_func, self.labels)))\n            total = np.sum(self.weights)\n            self.weights = (self.weights / total)\n\n    def __len__(self):\n        return len(self.indices) // self.batch_size\n\n    def __getitem__(self, index):\n        indices = np.random.choice(self.indices, size=self.batch_size,\n                                   replace=False, p=self.weights)\n        return self.__data_generation(indices)\n\n    def on_epoch_end(self):\n        pass\n\n    def __data_generation(self, indices):\n        x = np.empty((self.batch_size, self.sequence_size, *self.img_size))\n        preprocess_func = lambda im: Preprocessor.preprocess(os.path.join(self.img_dir, im + \".dcm\"))\n        if self.labels is not None:  # training phase\n            y = np.empty((self.batch_size, self.sequence_size, 5), dtype=np.float32)\n            for i, idx in enumerate(indices):\n                seq = self.list_ids[idx]\n                seq_labels = np.array(self.labels[idx])\n                seq_len = len(seq)\n                # if there is an any label = 1, set has_hemorrhage flag\n                has_hemorrhage = np.any(seq_labels[0])\n                imgs = map(preprocess_func, seq)\n                if has_hemorrhage:\n                    # augment images\n                    func_idxs = np.random.randint(0, self.n_augment + 1,\n                                                  size=seq_len)\n                    imgs = [self.augment_funcs[j](img)\n                            for j, img in zip(func_idxs, imgs)]\n                else:\n                    # consume map generator\n                    imgs = list(imgs)\n                imgs = np.array(imgs)\n                imgs = np.repeat(imgs[..., np.newaxis], 3, -1)\n                diff = seq_len - self.sequence_size\n                if diff < 0:\n                    padding = np.repeat(np.zeros(imgs.shape[1:])[np.newaxis, ...], abs(diff), 0)\n                    imgs = np.concatenate((imgs, padding), axis=0)\n                    seq_labels = np.concatenate(seq_labels, np.zeros(6), axis=0)\n                elif diff > 0:\n                    indices = get_sequence_clipping_order(seq_len)\n                    imgs = np.delete(imgs, indices[:diff], 0)\n                    seq_labels = np.delete(seq_labels, indices[:diff], 0)\n                x[i, ] = imgs\n                y[i, ] = seq_labels[:, 1:]\n            return x, y\n        else:  # test phase\n            for i, idx in enumerate(indices):\n                seq = self.list_ids[idx]\n                seq_len = len(seq)\n                imgs = np.array(list(map(preprocess_func, seq)))\n                imgs = np.repeat(imgs[..., np.newaxis], 3, -1)\n                diff = seq_len - self.sequence_size\n                if diff < 0:\n                    padding = np.repeat(np.zeros(imgs.shape[1:])[np.newaxis, ...], abs(diff), 0)\n                    imgs = np.concatenate((imgs, padding), axis=0)\n                elif diff > 0:\n                    indices = get_sequence_clipping_order(seq_len)\n                    imgs = np.delete(imgs, indices[:diff], 0)\n                x[i, ] = imgs\n            return x","execution_count":157,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"from keras.applications import NASNetLarge, InceptionResNetV2, Xception, DenseNet201, ResNet50\nfrom keras.layers import Bidirectional, LSTM, TimeDistributed, Masking\nfrom keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input, GlobalAveragePooling2D\nfrom keras.models import Sequential, Model\n\n\nclass StandardModel:\n\n    def __init__(self, network, input_shape, pooling_method='max', classes=2, use_softmax=True):\n        self.base_model = self.get_base_model(network, input_shape, pooling_method)\n        self.classes = classes\n        self.use_softmax = use_softmax\n        self.input_shape = input_shape\n\n    @staticmethod\n    def get_base_model(network, input_shape, pooling_method):\n        network = network.lower()\n        input_warning_message = 'WARNING! The input shape is not the default one!!! Proceeding anyway!'\n        if network == 'nas':\n            if input_shape != (331, 331, 3):\n                print_error(input_warning_message)\n            return NASNetLarge(input_shape=input_shape, include_top=False, pooling=pooling_method,\n                               weights=None)\n        elif network == 'inception':\n            if input_shape != (299, 299, 3):\n                print_error(input_warning_message)\n            return InceptionResNetV2(input_shape=input_shape, include_top=False, pooling=pooling_method,\n                                     weights='imagenet')\n        elif network == 'xception':\n            if input_shape != (299, 299, 3):\n                print_error(input_warning_message)\n            return Xception(input_shape=input_shape, include_top=False, pooling=pooling_method,\n                            weights='imagenet')\n        elif network == 'densenet':\n            if input_shape != (224, 224, 3):\n                print_error(input_warning_message)\n            return DenseNet201(input_shape=input_shape, include_top=False, pooling=pooling_method,\n                               weights='imagenet')\n        elif network == 'resnet':\n            if input_shape != (224, 224, 3):\n                print_error(input_warning_message)\n            return ResNet50(input_shape=input_shape, include_top=False, pooling=pooling_method,\n                            weights='imagenet')\n        else:\n            print_error(f'Invalid network name: {network}! Please choose from: \\n ')\n            return None\n\n    def build_model(self):\n        return self.build_binary_model() if self.classes == 2 else self.build_multi_class_model()\n\n    def build_binary_model(self):\n        model = Sequential()\n        model.add(self.base_model)\n        model.add(Dense(96))\n        model.add(BatchNormalization())\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(Dropout(0.3))\n        model.add(Dense(1, activation='sigmoid'))\n        return model\n\n    def build_multi_class_model(self):\n        return self.build_probability_model() if self.use_softmax else self.build_recurrent_model()\n\n    def build_probability_model(self):\n        model = Sequential()\n        model.add(self.base_model)\n        model.add(Dense(96))\n        model.add(BatchNormalization())\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(Dropout(0.3))\n        model.add(Dense(5, activation='softmax'))\n        return model\n\n    def build_recurrent_model(self):\n        inputs = Input(shape=(40, *self.input_shape))\n        time_dist = TimeDistributed(self.base_model)(inputs)\n        global_pool = TimeDistributed(GlobalAveragePooling2D())(time_dist)\n        dense_relu = TimeDistributed(Dense(256, activation='relu'))(global_pool)\n\n        masked = Masking(0.0)(dense_relu)\n        out = Bidirectional(LSTM(256, return_sequences=True, activation='softsign',\n                                 dropout=0.2, recurrent_dropout=0.2))(masked)\n        out = TimeDistributed(Dense(5, activation='softmax'))(out)\n\n        model = Model(inputs=inputs, outputs=out)\n        return model","execution_count":158,"outputs":[]},{"metadata":{"pycharm":{"is_executing":false,"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"import glob\nimport os\nimport sys\nimport time\n\nimport keras\nimport matplotlib.pyplot as plt\nfrom keras.optimizers import Adamax\nfrom sklearn.metrics import log_loss\nfrom keras.callbacks import ModelCheckpoint, Callback\n\nclass WeightsSaver(Callback):\n    def __init__(self, N):\n        self.N = N\n        self.batch = 0\n\n    def on_batch_end(self, batch, logs={}):\n        if self.batch % self.N == 0:\n            name = 'model_weights.h5'\n            self.model.save_weights(name)\n        self.batch += 1\n        \ndef prepare_data():\n    csv = pd.read_csv(os.path.join('../input/data-misc', 'labels.csv'))\n    files = glob.glob(os.path.join(TRAIN_DIR, \"*.dcm\"))\n    files = list(map(lambda x: os.path.splitext(os.path.basename(x))[0], files))\n    filtered_csv = csv[csv.id.isin(files)]\n    indices = np.random.rand(len(filtered_csv))\n    mask = indices < 0.8\n    x_train, y_train = list(filtered_csv[mask].id), filtered_csv.iloc[mask, 1:]\n    x_test, y_test = list(filtered_csv[~mask].id), filtered_csv.iloc[~mask, 1:]\n    #x_train.reset_index(inplace=True, drop=True)\n    y_train.reset_index(inplace=True, drop=True)\n    #x_test.reset_index(inplace=True, drop=True)\n    y_test.reset_index(inplace=True, drop=True)\n    return x_train, y_train, x_test, y_test\n\n\ndef prepare_sequential_data():\n    # open label + metadata CSV\n    csv = pd.read_csv(os.path.join('../input/data-combined', \"train_meta.csv\"))\n    # sort by study ID and position\n    csv.sort_values(by=[\"StudyInstanceUID\", \"ImagePositionPatient3\"], inplace=True, ascending=False)\n    label_columns = [\"any\", \"epidural\", \"intraparenchymal\",\n                     \"intraventricular\", \"subarachnoid\", \"subdural\"]\n    # filter unnecessary columns\n    csv = csv[[\"StudyInstanceUID\", \"id\"] + label_columns]\n    # get sequences of IDs (groupby preserves order)\n    sequences = csv.groupby(\"StudyInstanceUID\")[\"id\"].apply(list)\n    # group labels into one single column\n    csv[\"labels\"] = csv[label_columns].values.tolist()\n    # get sequences of labels\n    labels = csv.groupby(\"StudyInstanceUID\")[\"labels\"].apply(list)\n    indices = np.random.rand(sequences.size)\n    # partition data\n    mask = indices < 0.8\n    x_train, x_test = list(sequences.iloc[mask]), list(sequences.iloc[~mask])\n    y_train, y_test = list(labels.iloc[mask]), list(labels.iloc[~mask])\n    return x_train, y_train, x_test, y_test\n\n\ndef test_recurrent_network():\n    def generate_single_instance(instance):\n        images, labels = list(), list()\n        for file in instance:\n            file_path = os.path.join(TRAIN_DIR, file)\n            images.append(Preprocessor.preprocess(file_path))\n            labels.append(np.random.uniform(0, 1, 5))\n        images = np.stack(images, axis=0)\n        labels = np.stack(labels, axis=0)\n        return images, labels\n\n    model = StandardModel('xception', (512, 512, 3), classes=5, use_softmax=False, pooling_method=None)\n    model = model.build_model()\n    model.compile(Adamax(), loss='categorical_crossentropy', metrics=['acc'])\n    model.summary()\n    keras.utils.plot_model(model, show_shapes=True)\n    x_train = []\n    y_train = []\n    data = [['ID_00025ef4b.dcm', 'ID_00027c277.dcm', 'ID_00027cbb1.dcm'],\n            ['ID_000229f2a.dcm', 'ID_000230ed7.dcm', 'ID_000270f8b.dcm'],\n            ['ID_00025ef4b.dcm', 'ID_00027c277.dcm', 'ID_00027cbb1.dcm']]\n    for i in range(3):\n        instance_images, instance_labels = generate_single_instance(data[i])\n        x_train.append(instance_images)\n        y_train.append(instance_labels)\n    x_train = np.stack(x_train)\n    x_train = np.repeat(x_train[..., np.newaxis], 3, -1)\n    y_train = np.stack(y_train)\n    print(x_train.shape, y_train.shape)\n    model.fit(x_train, y_train, batch_size=1)\n\n\ndef plot_model_graph(history, graph_name):\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['loss'])\n    plt.title('Model Accuracy & Loss')\n    plt.ylabel('Accuracy & Loss')\n    plt.xlabel('Epoch')\n    plt.savefig(graph_name)\n\n\ndef train_binary_model(base_model, already_trained_model=None):\n    x_train, y_train, x_test, y_test = prepare_data()\n    if not already_trained_model:\n        nr_sequences = 30\n        sequence_length = len(x_train) // nr_sequences\n        model = StandardModel(base_model, (512, 512, 3), classes=2, use_softmax=True)\n        model = model.build_model()\n        model.compile(Adamax(), loss='binary_crossentropy', metrics=['acc'])\n        checkpoint = ModelCheckpoint('binary_model.h5', monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n        start_time = time.time()\n        estimated_window = None\n        for index in range(nr_sequences):\n            frame = (index * sequence_length, (index+1)*sequence_length)\n            labels = y_train[frame[0]:frame[1]].reset_index(drop=True)\n            history = model.fit_generator(DataGenerator(x_train[frame[0]:frame[1]], labels=labels,\n                                                        n_classes=2, batch_size=8), epochs=1)\n            plot_model_graph(history, \"my_binary_graph.png\")\n            model.save('binary_model.h5')\n            loss, accuracy = model.evaluate_generator(DataGenerator(x_test, labels=y_test, n_classes=2))\n            print(loss, accuracy)\n            if index == 0:\n                estimated_window = time.time() - start_time     # cat a durat un fit\n            if (time.time() + estimated_window) - start_time >= 28800:   # daca timpul estimat este mai mare de 8 ore, ii spunem adio\n                break\n    else:\n        if os.path.exists(already_trained_model):\n            model = keras.models.load_model(already_trained_model)\n            loss, accuracy = model.evaluate_generator(DataGenerator(x_test, labels=y_test, n_classes=2))\n            print(loss, accuracy)\n        else:\n            print_error(\"Provided model file doesn't exist! Exiting...\")\n            sys.exit(1)\n\n\ndef train_multi_class_model(base_model, already_trained_model=None):\n    x_train, y_train, x_test, y_test = prepare_data()\n    if not already_trained_model:\n        nr_sequences = 30\n        sequence_length = len(x_train) // nr_sequences\n        model = StandardModel(base_model, (512, 512, 3), classes=5, use_softmax=True)\n        model = model.build_model()\n        model.compile(Adamax(), loss='categorical_crossentropy', metrics=['acc'])\n        checkpoint = ModelCheckpoint('categorical_model.h5', monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n        start_time = time.time()\n        estimated_window = None\n        for index in range(nr_sequences):\n            frame = (index * sequence_length, (index+1)*sequence_length)\n            history = model.fit_generator(DataGenerator(x_train[frame[0]:frame[1]], labels=y_train[frame[0]:frame[1]], \n                                                        n_classes=5, batch_size=8), epochs=1)\n            plot_model_graph(history, \"my_categorical_graph.png\")\n            model.save('categorical_model.h5')\n            y_pred = model.predict_generator(DataGenerator(x_test, n_classes=5))\n            y_test = y_test.iloc[:, 1:]\n            print(log_loss(y_test, y_pred))\n            if index == 0:\n                estimated_window = time.time() - start_time              # cat a durat un fit\n            if (time.time() + estimated_window) - start_time >= 28800:   # daca timpul estimat este mai mare de 8 ore, ii spunem adio\n                break\n    else:\n        if os.path.exists(already_trained_model):\n            model = keras.models.load_model(already_trained_model)\n            y_pred = model.predict_generator(DataGenerator(x_test, n_classes=5))\n            y_test = y_test.iloc[:, 1:]\n            print(log_loss(y_test, y_pred))\n        else:\n            print_error(\"Provided model file doesn't exist! Exiting...\")\n            sys.exit(1)\n\n\ndef train_recurrent_multi_class_model(base_model, already_trained_model=None):\n    x_train, y_train, x_test, y_test = prepare_sequential_data()\n    if not already_trained_model:\n        model = StandardModel(base_model, (512, 512, 3), classes=5, use_softmax=False, pooling_method=None)\n        model = model.build_model()\n        model.compile(Adamax(), loss='categorical_crossentropy', metrics=['acc'])\n        model.fit_generator(LSTMDataGenerator(x_train, labels=y_train))\n        model.save('model.h5')\n        y_pred = model.predict_generator(LSTMDataGenerator(x_test))\n        print(log_loss(y_test, y_pred))\n    else:\n        if os.path.exists(already_trained_model):\n            model = keras.models.load_model(already_trained_model)\n            y_pred = model.predict_generator(LSTMDataGenerator(x_test))\n            print(log_loss(y_test, y_pred))\n        else:\n            print_error(\"Provided model file doesn't exist! Exiting...\")\n            sys.exit(1)\n\n\ndef main():\n    # TODO: Possible MODELS for training: inception, xception, resnet, densenet, nas\n    # train_binary_model('xception')\n    # train_multi_class_model('xception')\n    # prepare_sequential_data()\n    train_binary_model('xception')\n    #test_recurrent_network()\n\nmain()\n","execution_count":159,"outputs":[{"output_type":"stream","text":"\u001b[95mWARNING! The input shape is not the default one!!! Proceeding anyway!\u001b[0m\nEpoch 1/1\n2246/2246 [==============================] - 1435s 639ms/step - loss: 0.2848 - acc: 0.8846\n0.09722843766212463 0.9037972092628479\nEpoch 1/1\n 408/2246 [====>.........................] - ETA: 19:32 - loss: 0.2479 - acc: 0.9044","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-159-db06cd6ee640>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m#test_recurrent_network()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-159-db06cd6ee640>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# train_multi_class_model('xception')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# prepare_sequential_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mtrain_binary_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xception'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0;31m#test_recurrent_network()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-159-db06cd6ee640>\u001b[0m in \u001b[0;36mtrain_binary_model\u001b[0;34m(base_model, already_trained_model)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             history = model.fit_generator(DataGenerator(x_train[frame[0]:frame[1]], labels=labels,\n\u001b[0;32m--> 118\u001b[0;31m                                                         n_classes=2, batch_size=8), epochs=1)\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0mplot_model_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"my_binary_graph.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'binary_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}}},"nbformat":4,"nbformat_minor":1}